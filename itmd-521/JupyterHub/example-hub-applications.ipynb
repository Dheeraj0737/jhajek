{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Removing hard coded password - using os module & open to import them from creds.txt file\n",
    "import os\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    creds_file = (open(f\"/home/{os.getenv('USER')}/creds.txt\", \"r\")).read().strip().split(\",\")\n",
    "    accesskey,secretkey = creds_file[0],creds_file[1]\n",
    "except:\n",
    "    print(\"File not found, you can't access minio\")\n",
    "    accesskey,secretkey = \"\",\"\"\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.2.3')\n",
    "conf.set('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')\n",
    "\n",
    "conf.set('spark.hadoop.fs.s3a.access.key', accesskey)\n",
    "conf.set('spark.hadoop.fs.s3a.secret.key', secretkey)\n",
    "# Configure these settings\n",
    "# https://medium.com/@dineshvarma.guduru/reading-and-writing-data-from-to-minio-using-spark-8371aefa96d2\n",
    "conf.set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "# https://github.com/minio/training/blob/main/spark/taxi-data-writes.py\n",
    "# https://spot.io/blog/improve-apache-spark-performance-with-the-s3-magic-committer/\n",
    "conf.set('spark.hadoop.fs.s3a.committer.magic.enabled','true')\n",
    "conf.set('spark.hadoop.fs.s3a.committer.name','magic')\n",
    "# Internal IP for S3 cluster proxy\n",
    "conf.set(\"spark.hadoop.fs.s3a.endpoint\", \"http://system54.rice.iit.edu\")\n",
    "# Send jobs to the Spark Cluster\n",
    "conf.setMaster(\"spark://sm.service.consul:7077\")\n",
    "#Set driver and executor memory\n",
    "conf.set(\"spark.driver.memory\",\"4g\")\n",
    "conf.set(\"spark.executor.memory\",\"4g\")\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Your App Name\")\\\n",
    "    .config('spark.driver.host','spark-edge.service.consul').config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now Reading a dataset from Minio Object Storage and doing string manipulation\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "df = spark.read.csv('s3a://itmd521/50.txt')\n",
    "\n",
    "splitDF = df.withColumn('WeatherStation', df['_c0'].substr(5, 6)) \\\n",
    ".withColumn('WBAN', df['_c0'].substr(11, 5)) \\\n",
    ".withColumn('ObservationDate',to_date(df['_c0'].substr(16,8), 'yyyyMMdd')) \\\n",
    ".withColumn('ObservationHour', df['_c0'].substr(24, 4).cast(IntegerType())) \\\n",
    ".withColumn('Latitude', df['_c0'].substr(29, 6).cast('float') / 1000) \\\n",
    ".withColumn('Longitude', df['_c0'].substr(35, 7).cast('float') / 1000) \\\n",
    ".withColumn('Elevation', df['_c0'].substr(47, 5).cast(IntegerType())) \\\n",
    ".withColumn('WindDirection', df['_c0'].substr(61, 3).cast(IntegerType())) \\\n",
    ".withColumn('WDQualityCode', df['_c0'].substr(64, 1).cast(IntegerType())) \\\n",
    ".withColumn('SkyCeilingHeight', df['_c0'].substr(71, 5).cast(IntegerType())) \\\n",
    ".withColumn('SCQualityCode', df['_c0'].substr(76, 1).cast(IntegerType())) \\\n",
    ".withColumn('VisibilityDistance', df['_c0'].substr(79, 6).cast(IntegerType())) \\\n",
    ".withColumn('VDQualityCode', df['_c0'].substr(86, 1).cast(IntegerType())) \\\n",
    ".withColumn('AirTemperature', df['_c0'].substr(88, 5).cast('float') /10) \\\n",
    ".withColumn('ATQualityCode', df['_c0'].substr(93, 1).cast(IntegerType())) \\\n",
    ".withColumn('DewPoint', df['_c0'].substr(94, 5).cast('float')) \\\n",
    ".withColumn('DPQualityCode', df['_c0'].substr(99, 1).cast(IntegerType())) \\\n",
    ".withColumn('AtmosphericPressure', df['_c0'].substr(100, 5).cast('float')/ 10) \\\n",
    ".withColumn('APQualityCode', df['_c0'].substr(105, 1).cast(IntegerType())).drop('_c0')\n",
    "\n",
    "splitDF.printSchema()\n",
    "splitDF.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the placeholder value to be your HAWKID, which is your bucketname\n",
    "\n",
    "splitDF.write.mode(\"overwrite\").option(\"header\",\"true\").csv(\"s3a://REPLACEWITHYOURHAWKID/50.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to hold output even after the Jupytr Hub web page closes\n",
    "### If the job submitted takes some time to run we can generally use nohup to redirect the output without having us to keep the ssh session alive. As, we are not submitting the job's via terminal there is no way to use no hup directly. To get around this issue, we will use `%%capture` magic to capture the output.\n",
    "\n",
    "### Run the below code in a new cell. `%%capture myapp` has been added in the first line. (Replace `myapp` with a better name.)\n",
    "\n",
    "%%capture myapp\n",
    "avg_df = splitDF.select(month(col('ObservationDate')).alias('Month'),year(col('ObservationDate')).alias('Year'),col('AirTemperature').alias('Temperature'))\\\n",
    "             .groupBy('Month','Year').agg(avg('Temperature'),stddev('Temperature')).orderBy('Year','Month')\n",
    "\n",
    "avg_df.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last line required to release the Spark resources after finished executing\n",
    "\n",
    "### It is recommended that you restart the kernel once you stop the session before starting a new session by clicking restart kernel situated right of stop button (or) from kernel menu, as it clears all the cached variables.\n",
    "\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
